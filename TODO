TODO:

- BUG: first command in new single inprocess IPython widget doesn't print results, subsequent ones work fine
    - this seems to be limited to inprocess kernel (not zmq), and has something to do with calling ipythonqtwidget.execute('blah', hidden=True). It's the hidden kwarg that seems to cause this problem.

- add File/Run dialog to choose a python file to run interactively with %run -i

- maybe replace all instances of dt* with td*? td is probably a better acronym for "time duration" than dt is for "duration". td might be mistaken for time delta, but that's not really any different from time duration

- plot log firing rate distributions in meanratehist() plots

- use new Ipython's new InProcessKernel to improve IPython integration - hopefully this will finally allow dropping into the ipython debugger in the qt widget on any error while running neuropy?
    - https://github.com/ipython/ipython/pull/2724

- to auto activate pdb, try some of these:
    - search for "exc" in:    http://ipython.org/ipython-doc/dev/api/generated/IPython.core.interactiveshell.html#IPython.core.interactiveshell.InteractiveShell.set_custom_exc
        - call_pdb
    
    - or, from http://mail.scipy.org/pipermail/ipython-dev/2012-April/008945.html:
    # define custom exception handler:

    def custom_exc(shell, etype, evalue, tb, tb_offset=None):
        # do you own thing:
        print 'hi', etype
        # also do what IPython would have done, if you want:
        shell.showtraceback((etype, evalue, tb), tb_offset=tb_offset)
        # or drop into pdb or ipdb?

    # Tell IPython to use it for any/all Exceptions:
    get_ipython().set_custom_exc((Exception,), custom_exc)

- add ability to report nids of pairs for a given point in any of the cc() plots - tooltip? replace or augment the status bar printout on hover? Will presumably need to deal with MPL hover events, or at a lower level, with qt4 hover events

- calculate average xcorr for sets of pairs

- add mean, median and mode pairs (for indep and pairwise models) to NetstateDJSHist, as I have for CodeCorrPDFs. And/or, show differences between them for the two models. Problem is the legend is in the top right, so I have to position the text appropriately below it. Should really add other stuff like tres, phase, R, and minrate.
    - maybe do ngroups = 1000 for more confidence

- add ability to save (pickle) any object to file, using file save dialog, accessed via menu and toolbar. Will the scope work? Don't think the Qt window can access the ipython stuff, can it? Maybe I can. Or, just make a pickle command that you pass the object name, and then brings up a Save As dialog box.
    - then also need ability to restore the pickle, and assign it to a name in the workspace
    - looks like at least some objects (like NetstateDJSHist) have a lot of unpicklable things in them, would need to create __getstate__ methods...
    - wow! apparently MPL 1.2 has figure pickling support! That would be almost as good...

- add a different type of ns_scatter, which plots measured rates of occurence of all netstates during one stimulus vs that of the other. Also, have ability to split a stimulus up either in half in time or randomly, and plot netstate occurence rates of one half vs the other

- add a NetstateDJSScatter. As for many other analyses, instead of plotting distributions, do a scatter plot of the points that make up a NetstateDJSHist, with indep value of DJS of a group of neurons on x axis and pairwise model value of DJS on y axis. Again, this shows you how the DJS of each group of neurons is changing, and by how much, between the two models.

- NetstateDJSHist plots aren't the same size as NetstateScatter plots for some reason... make sure all plots come out the same size, including CodeCorrPDFs

- check if there's a way to programmatically make all figure PDFs have a transparent background
    - I thought I looked into this a while ago. Was it really such a desirable thing?

- add option to pass set of neuron groups to NetstateDJSHist, so you can directly compare the result for two different recordings. To do this, all groups would have to be a subset of the intersection of active neurons in both recordings

- add ability to split recordings in half on the fly, or at least pass custom tranges down to analyses called on a recording
    - ah! Recording.codecorr already has a tranges arg! But does it work properly?
    - Recording.codecorrpdf could use a tranges arg as well
    - or, instead of splitting, could interleave half the time bins for one axis, and use the rest for the other axis. This would give a different (better?) estimate of the amount of noise in the analysis within a recording, compared to splitting temporally in half
        - use a kwarg interleave

- confirm that measuring correlations from Ising matrix gives similar results to measuring peaks in cross correlograms. Possible this may have not been done by anyone before.

- add a CodeCorrScatter to scatter pairwise correlations against each other, as possibly a more informative way of showing how correlated the correlations are, ie if they generally remain constant between the two periods of recording, if they've all generally gotten bigger or smaller, or if they've become completely scrambled

- figure out why the edges of bars in histograms show up in PDFs saved from MPL, but not in the, say QtAgg MPL backend. Some kind of minor bug in MPL's PDF backend?

- are correlations measured with 0 and 1 binary codes, or -1 and 1? This could affect correlation values. Note that the codes fed to the maxent model are -1 and 1.

- plot distributions of and scatter plots of delayed correlations, ie the correlations fed to the dynamic Ising model (Roudi analysis). Working with these would double the number of pairs compared to simultaneous spike correlations, because for these, order matters. A->B != B->A

- it would be nice to show that pairwise corr distributions for binned and digitized spike trains are the same as those for purely binned spike trains (see thought process below)

- maybe I should measure skew in addition to mean, median, mode of corrcodepdf distribs

- use custom IPython log file sepcific to neuropy, so that commmands from user's other IPython instances don't show up in neuropy, and vice versa

- add ability to concatenate recordings on the fly, by making tr1.r5_r6 to represent the concatenation of recordings 5 and 6, for example
    - concatenate the spike times, but then also have to do something with the experiment objects - make a new one that concatenates the experiments of all the specified recordings? ie just make a single big merged .din equivalent?

- tuning curves
    - add Experiment.tune() method which creates a Tune object for each child neuron and combines results into a single scrollable window, like sta? - this would be lower priority
    - allow multiple dimension tuning plots
        - the first one is the x axis, and all the other ones are expressed as overplots?
        - could also do 3D plots so you could clearly see two dimensions at a time
        - could even do 3 dims in 3D, some sort of volume plot with transparency, using colour to denote response strength
    - maybe make tdelay arg a list and do overplots in different colours for each tdelay, a different colour for each
    - psths for specific values of chosen dimension?
    - raster plots split up according to all DIN, or according to just one dimension
    - ability to specify time range, in fraction or in seconds, of each sweep to consider when couting up spikes during that sweep. Eg, useful for driftbar where there's only a response in the middle 1-2 sec that you're interested in, but might be a fair bit of baseline outside of that
    - error bars on tuning curves: how to calculate them, especially when collapsing across multiple dimensions? I guess I should calculate the mean and stdev of spikes per presentation of each sweep, and then plot the SEM as the error bar for that condition (take stdev / sqrt(n) or sqrt(n-1))
        - instead of accumulating spike count for each din, should save spike rate for *each* din. Then, when collapsing over some dimensions, or keeping some fixed, take mean of all those spike rate values (assuming all din are of equal duration, if not, weight by inverse of each din's duration), and also take stdev to get SEM
        - it would be good to have at least the option of plotting spike counts instead of rates 
    - fit single and double gabors to single and double-peaked tuning curves
        - use von Mises function, width is indirectly related to some k parameter, fit it using LM
        - or, use wrapped Guassians, has nicer definition of width, fit using LM

- codes are not currently constrained to when stimuli are on the screen, although this shouldn't be a big deal most of the time. See recording.BaseNetstate.codes()

- why bother with the digitization of spike trains? Why not leave the number of spikes in each bin instead of truncing it at 1? You'll end up with a signal that's like 0 0 0 0 0 1 0 0 0 2 0 0 1 0 1 0 3 0, ie mostly 0s anyway. And, it saves more information about each spike train, which should then allow for more accurate correlation calculations between pairs. Seems to me the only really important thing is binning, not digitization. Or am I forgetting something? Ah, it prevents from forming meaningful digital words across neurons (space). So, you could digitize for doing netstate stuff, but leave undigitized for simply calculating pairwise corrs? Not really, cuz you need to feed the pairwise corrs into the maxent model, and those would have to correspond to the word probabilities, ie the pairwise corrs would have to be calculated off the binary signals. But, at least it would be nice to show that pairwise corr distributions for binned and digitized spike trains are the same as those for purely binned spike trains.

- consider switching over from the deprecated scipy.maxentropy to the maxent library:
    http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html

----------------------------------------------
OLD:

- remove all wx stuff, replace with qt stuff
    - can't seem to get Qt window creation to work from within neuropy code. Need to examine and mimic exactly how the kernel manager deals with mpl qt plot calls to create a qt window in parallel thread
    - might not be necessary in the end. Could just do everything through mpl

- convert all line endings to Unix style

- don't load all of a recording's sorts, just the default one

- when do I ever actually make use of the _movies dictattr (or previously, the _data.movies dictattr)? Am I really preventing movies from being loaded more than once? I used to use it in the commented out core.Cat15Movie(), but the dimstimskeletal.Movie() doesn't seem to use it. Is it even worth the hassle? I guess so. Every loaded recording that used a movie would otherwise load and hold its own copy of potentially the same movie data. Looks like the fake dimstim 0.16 experiment in loadCat15exp() also doesn't make use of it. Or maybe it happens to magically during the textheader execution that I'm not seeing it right now

- allow all objects to have None as parents. this way you can load a recording all on its lonesome, without having to searching up and down the tree in a complicated error prone way for its parent. Like, what if you just have a single recording directory sitting somewhere on your hard drive? Should still be able to load it all by its lonesome as say "r32" name in the namespace

- replace move file paths in read back textheaders with local movie file path as defined by core.MOVIEPATH - right now I've simply made a symbolic link in root called /mov which points to /home/mspacek/data/mov

- STA is really slow for some reason, or maybe this only happens in pyshell
    - reduce din entries down to unique values - ie get rid of repeated frame values, to make searchsorted faster - for a 40ms mseq, you'd go from 8 entries per frame (5 ms each) down to just one
    - also, maybe reduce mseq black and white pix values from 0, 255 to 0, 1, so you can accumulate in a smaller sized data type (maybe pure int instead of float, for int speed)
    - also maybe keep in mind what NVS said about finding average of a large number of floats by first adding them up and then dividing at the end: you lose precision because of the limited mantissa. Hopefully I'm adding up ints anyway, so this shouldn't apply

- auto drop into ipdb in the Qt widget on any error encountered in the ipython kernel during execution

- the whole ContrainedNeuron thing seems pretty retarted to me after not looking at it for a while... Is it really necessary? Why are there two separate sets loaded for each rip (both Neurons *and* ConstrainedNeurons)?

- change best rip from being part of foldername (which causes problems when renaming folders with svn when you wanna temporarily change which rip folder is used for analysis) to just being an empty file called "BEST" or something within that rip's folder, or maybe a text file called "DEFAULT" in the Track's folder that has the default rip folder's name in it
    - this won't work once we start replacing rip folders with .sort files...
    - maybe by default use the most recently generated .sort folder (as designated by its now auto-generated name with the datetime stamp in it), or if any of them have a DEFAULT file inside, use that one as an override
    - if .sort files exist in the path, and none of the .sort folders have a DEFAULT file inside them, use the most recently generated .sort file (again, as designated by the datetime stamp in its filename)

- NVS: xcorr RFs, find pairs that have very correlated RFs, then check if those two cells really are different cells, according to their spike waveforms. If waveforms really are different, then compare responses to natural scenes. This way, you can test if an mseq RF is in any way useful for predicting responses to nat scenes. If responses are different to nat scenes, yet RFs are the same, then visual neuroscience is hosed!

- NVS: normalize so that Rfs have the same 0 point, ie same 0 colour

- changing the global default, say, ANIMAL in neuropy.Core on the fly doesn't seem to work

- get rid of all use of super(), replace with direct call of base class instead

- limit each Recording to only one Sort object, which is by default either the one that says 'best' in its filename, or the first one alphabetically. Override default by specifying the sorts filename as an arg to Recording.__init__ and/or .load(). Maybe add a Recording.get_sortnames method that lists available sort filenames (minus their .sort extension)
    - if you want to work with multiple Sorts at the same time, create multiple Recordings, each with a different Sort
    - this needs to work with the sort (formerly rip) folders as well, for data exported from SurfBawd

- maybe the object hierarchy or some functionality within it could be changed somehow to clarify that neuron x from Recording y is the same as neuron x from Recording z. Need to rely on the same IDs being assigned during spike sorting, but would be nice if this was then reconciled in neuropy

- use the .picker attrib of an artist (like a line) to specify whether it first a pickevent when clicked on within a certain pixel range, use this for the Schneidman scatter plots, much much easier. See pick_event_demo.py in mpl examples, or spyke.plot

- could use "fnmatch" module to do filename matching in the data folders

- add code to load .sort files

- since I'm now shuffling mseq frames in dimstim experiments, check to make sure STA code doesn't assume mseq frames are in numerical order (which I'm almost certain it doesn't). If it does, this would help explain why I haven't been finding mseq fields in rat

- tuning curves for various parameters (ori, sfreq, tfreq, phase,...), for driftbars and gratings, for both Cat 15 and dimstim 0.16
    - this should probably be some kinda TuningCurve object, analogous to all the other analysis objects I've got

- from __future__ import division

- get rid of all use of python's built-in random module, use numpy.random instead

- if an analysis method needs to return more than one object, have them return in a dictattr of results


Netstate stuff:
    - for the time bins, try lots of different phases, see if this changes things at all
        - look if the most common states remain the most common
    - since Recordings are fairly short, either combine lots of Recordings into one (or their Experiments), or use a shorter CODEWORDLENGTH to get better stat significance for all possible words
        - add append() method to Experiment? no. din values have different meaning between Experiments
        - better to add append() to Recording? yes.
        - best to just use append() in Neuron? yes. Recording.append() will make use of this. make .spikes 2d array? no, offsets have been added
    - for common states, see if there's a gradual falloff of probs of being 1 bit, 2bit 3 bit off of that state as you move away from it in bitspace
        - maybe map out some kind of attractors in bitspace
    - maybe mess around with higher bit codes than just binary, like trinary, quaternary, which reflect more accurately the number of spikes in each bin for each neuron
    - rename all instances of binarray to binmatrix (or binmat) in cases where it's 2D (which is most of the time)
    - make plotnspikingPMFs use nsamples so you get a nice average with errorbars

- make Neuron.append() in place, and ensure whenever it's used, that's it's used on a copied Neuron
- do analyses across Recordings, using Neuron.append(), to increase significance

- add checks to Experiment.load() to see if we're dealing with Cat 15 textheaders, or the newer dimstim.SweepTable class!!
    - need to keep old buildSweepTable() f'n in dimstim.Core for backward compatibility

- replace all SLASH, / and \\ stuff by using os.path calls, like os.path.basename, os.path.dirname, os.path.join, os.path.normpath, os.path.splitdrive, os.path.splitext, os.path.split
- add a Lab object to data hierarchy, need to bump everyone else's level down one (up in number, self.level +=1)


- MPL:
    - when saving figs, automatically fill in the file name with the text of the title bar of the figure

- cross-correlograms
    - make it faster
    - add p values to peaks (% of ticks within the peak, out of all the ticks in the window)
    - do both types of shuffle correction (those would be ?)

- plot 2D matrix of cross-correlograms - see the README.wx file in mpl/examples for embedding in wx

- PSTHs
    - look more closely for 5 ms peaks. Also, looks at data recorded at lower screen refresh rates

- spike interval histograms with log scale (see Lundstrom + Fairhall 2006)

- LFPs! how to export them from surfbawd, and handle them in neuropy? forget that, access them directly with spyke

- rasters
    - multi trial single neuron raster plots
    - make rasters faster when large number of spikes on screen (instead of deleting and recreating all vlines, do so just for ones that disappear and appear?)
    - get scrolly wheel detection to zoom in and out (not possible using mpl events? have to go to wx events?)

- STC
- revcorr to sparse bars, or any stimulus really, by directly sampling VisionEgg's framebuffer - easy! just use screen.get_framebuffer_as_array - see r72 makesparsemovie file. Need to refactor Dimstim (new version, call it lowercase dimstim?) into more OOP to really do this nicely
- maybe change experiment names to include only everything after the exp id in the .srf filename, prevents cluttery repetition of recording name. In case there's only one experiment, use the full .srf filename less the leading recording id and - at the start?
    - how would this change affect an experiment name in say an rf_mapping recording, like r75?


- make PyShell/PyCrust log user input to a file
- Nah, not important?: Rips should really have ids to make them easier to reference to: r[83].rip[0] instead of r[83].rip['conservative spikes'] - this means adding id prefixes to rip folder names (or maybe suffixes: 'conservative spikes.0.rip', 'liberal spikes.1.rip', etc...). Prefixes would be better cuz they'd force sorting by id in explorer (which uses alphabetical order) - ids should be 0-based of course
- worry about conversion of ids to strings: some may be only 1 digit and may have a leading zero!
- maybe make two load() f'ns for Experiment and Neuron: one from files, and a future one from a database
- make a save() f'n that pickles the object (including any of its results, like its STA, tuning curve points, etc)? - just use IPython's %store

- more detailed experimental info:
    - Recordings
        - maybe add other info about the Recording, stored in the same folder, like skull coordinates, angles, polytrode name and type...
        - LFPs
            - maybe a .lfp binary file, one per lfp channel, with alternating timestamps and voltage (uV?), ie (int64, float64) pairs
    - Rips
        - then, maybe add something that loads info about the rip, say from some file describing the template used, and all the thresholds, exported to the same folder by SURF
        - maybe also load the template file used for the rip, perhaps also stored in the same folder
    - Neurons
        - then, maybe add something that loads the template for this neuron, as well as its modelled (or just guesstimated) location in 3D (or just 2D) coordinates in um, as well as cell type potentially





---------------------------------------
DONE:

- STAs
- codes
- Netstate stuff
    - add codeword (binary and int) popup on float over a point in the scatter plot

- cross-correlograms
- population rasters
    - add neuron id as a popup or something on mouseover on population raster plot
- various rate methods, ratePDFs
- lots of other stuff I've forgotten about
- can replace all '%s' % repr(x) with just '%r' % x
- figure out how to grab the last command typed at the interpreter, so you can set that as the figure caption, makes things nice and explicit. Current code in various gcfm().frame.SetTitle calls that sets the caption sort of guesses what was typed is a hack
- increase precision of x, y coord display in statusbar of MPL figures

MPL: - when saving figs, automatically choose .png from list

- move code to /neuropy subfolder, make a setup.py in root, keep TODO in root
- make objects in hierarchy directly accessible in parent's namespace, if that name isn't already taken.
    - e.g. ptc15.t7c.r81 instead of having to type ptc15.t['7c'].r[81]


Netstate stuff:
    - look for check cells
    - do the maximum entropy Ising model
    - see if cortical data extrapolates to the same sort of ideal network size, ~200 neurons


- need to deal with change of dimstim movie sweep time from sweeptimeMsec to sweepSec for Cat 16+
- neuron shortcut names in Recording don't seem to exist. Only exist in rips?
- _data.movies is empty after doing sta on rat data, not so after doing it on ptc15
- rename Rip to Sort, and .rip to .sort, to jive with new .sort file and (Sort)Session object in spyke
- should really get rid of annoying dependency on dimstim. That would greatly ease installation - done, except for where it's really necessary, when trying to load an Experiment object generated by dimstim >= 0.16
- get rid of dependency on dimstim, which has unavoidable dependency on VisionEgg, which requires PyOpenGL and pygame libs, which create a big headache
    - need to redefine skeletal set of classes in neuropy so you can get parsing of textheaders to work - this really just requires a bunch of classes inheriting from dictattr, I think
    - also need to redefine SweepTable - depends on Dimension - don't need all the sweeptable code, just the part that actually builds the table
    - comment out all textheader lines starting with "from dimstim"
    - need to define all possible types of experiments: Movie, Grating, etc..
    - need to define StaticParams, DynamicParams, Variable, Variables, Runs, BlankSweeps
- stop using "import *"
    
Tuning curves:
    - Tune object, child of Neuron - done
    - ability to fix certain parameters when building tuning curves, something like:
        neuron.tune(var='phase0', fixed={'ori':[30, 45, 60], 'sfreqCycDeg':[0.8]})
        - don't forget that when calculating which sweepis to use, to subtract orioffset from the specified fixed ori, if ori is one of the fixed vars

- call figure.tight_layout(pad=0.3) on all generated figures to reduce whitespace around edges, better to be nice and cropped when importing into latex docs
- move CODEKIND, CODETRES, CODEPHASE, and CODEWORDLEN to globals, remove them as keywords everywhere else
- add pair depth to cc.shifts plots
- copy xcorr code from spyke to neuropy
